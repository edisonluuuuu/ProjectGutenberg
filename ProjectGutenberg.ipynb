{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import gutenberg\n",
    "import nltk\n",
    "import random\n",
    "from ast import literal_eval\n",
    "\n",
    "\n",
    "# This package contains a variety of scripts to make working with \n",
    "# the Project Gutenberg body of public domain texts easier.\n",
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.cleanup import strip_headers\n",
    "from gutenberg.query import get_etexts\n",
    "from gutenberg.query import get_metadata\n",
    "from gutenberg.query import list_supported_metadatas\n",
    "\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import (RandomForestClassifier, VotingClassifier)\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Before use one of the gutenberg.query functions, we must populate the local metadata cache. \n",
    "from gutenberg.acquire import get_metadata_cache\n",
    "cache = get_metadata_cache()\n",
    "cache.populate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I download all the data by using the package gutenberg. This package contains a variety of scripts to make working with the Project Gutenberg body of public domain texts easier. There are 100 observations in training data as for each of the ten famous authors I choose, ten English works are selected. This makes up the corpus that would be further processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 10 authors are selected. They are given a unqiue id from 1 to 10.\n",
    "authors = ['Shakespeare, William', 'Dickens, Charles', 'Twain, Mark', 'Verne, Jules', 'Austen, Jane', 'Poe, Edgar Allan', 'Henry, O.', 'Melville, Herman', 'Hawthorne, Nathaniel', 'Wharton, Edith']\n",
    "authors_id = [1,2,3,4,5,6,7,8,9,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare, William{1536, 1537, 23042, 23043, 1527, 1538, 1539, 1540, 1541, 23041, 1543, 1544, 1545, 1546, 23045, 23046, 1041, 1045, 1126, 10281, 12842, 45128, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 100, 1125, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 27761, 1137, 26224, 38901, 49297, 26268, 28334, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 23044, 1765, 1768, 1769, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 22791, 1799, 1800, 1801, 1802, 12578, 47960, 10606, 49007, 49008, 9077, 50559, 23935, 1430, 49146, 47518, 8609, 23970, 12719, 12720, 12721, 12722, 12723, 12724, 50095, 47715, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 24036, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1521, 1522, 1523, 1524, 1525, 1526, 1520, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535}\n",
      "Dickens, Charles{9728, 9729, 9730, 9731, 9732, 9733, 9734, 9735, 9736, 9737, 9738, 9739, 9740, 9741, 9742, 9743, 9744, 49683, 1023, 46, 19505, 564, 580, 588, 46675, 98, 43111, 644, 650, 653, 50334, 30368, 675, 676, 678, 699, 700, 7869, 20673, 43207, 37581, 35536, 23765, 730, 42232, 25852, 25853, 25854, 766, 37121, 15618, 49927, 1289, 41739, 9721, 786, 40723, 27924, 2324, 40729, 807, 809, 810, 23344, 821, 824, 20795, 872, 1392, 1394, 882, 883, 1400, 888, 1406, 1407, 25985, 1413, 1414, 1415, 1416, 19337, 1419, 1421, 1422, 1423, 912, 914, 916, 917, 918, 922, 1435, 924, 52125, 23452, 927, 8608, 41894, 47529, 47530, 47531, 47534, 30127, 47535, 22449, 1465, 1467, 963, 967, 968, 24022, 9689, 9690, 9691, 9692, 9693, 9694, 9695, 9696, 9697, 9698, 9699, 49125, 9702, 9703, 9704, 9705, 9706, 9707, 9708, 9709, 9710, 9711, 9712, 32241, 9713, 9714, 9715, 9716, 9717, 9719, 9720, 9718, 9722, 9723, 9724, 9725, 9726, 9727}\n",
      "Twain, Mark{2572, 19987, 1044, 7193, 7194, 7195, 19484, 7196, 7197, 7198, 7200, 7199, 19506, 5688, 5689, 5690, 5691, 5692, 5693, 1086, 32325, 70, 7242, 7243, 7244, 7245, 7246, 7247, 7248, 7249, 7250, 76, 86, 26203, 91, 93, 3171, 3172, 26213, 102, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 28781, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 119, 3191, 3192, 3194, 3195, 3196, 3193, 3189, 3190, 3200, 3197, 3198, 28803, 3199, 26252, 142, 5782, 5783, 5784, 5785, 5786, 5787, 5788, 3188, 5808, 5809, 3250, 5811, 5812, 3251, 5813, 5810, 5814, 19640, 5818, 5819, 5820, 1213, 5821, 5822, 26304, 5823, 5824, 20595, 5836, 5837, 5838, 5839, 5840, 5841, 5842, 19682, 245, 8471, 8472, 8473, 8474, 8475, 8476, 8477, 8478, 8479, 8480, 8481, 8482, 9001, 9002, 9003, 9004, 1837, 9005, 9006, 9007, 9008, 9009, 9010, 9011, 33077, 9012, 9013, 9014, 9015, 2874, 2875, 9016, 9017, 9018, 9019, 9020, 9021, 9022, 9023, 9024, 9025, 9026, 9027, 9028, 9029, 9030, 9031, 9033, 8525, 8526, 2895, 8527, 8528, 9034, 9035, 9036, 9037, 9038, 9039, 9040, 9041, 9042, 1892, 3432, 74, 2431, 19841, 9032, 7556, 6533, 8582, 8583, 8584, 8585, 8586, 8587, 8588, 8589, 8611, 12711, 7100, 50109, 7101, 7102, 7103, 7104, 7105, 7106, 7107, 7154, 7155, 7156, 7157, 7158, 7159, 7160, 7161, 7162}\n",
      "Verne, Jules{46597, 19362, 6538, 9618, 3091, 12051, 28947, 8979, 8980, 8981, 8982, 8983, 8984, 8985, 8986, 8987, 8988, 20000, 8989, 26658, 2083, 1698, 3748, 164, 8990, 8991, 8992, 8993, 11556, 18857, 1842, 10547, 25784, 19513, 2488, 9150, 23489, 3526, 16457, 24777, 1353, 32972, 1355, 83, 16085, 13527, 16344, 3808, 3809, 10339, 12901, 29413, 103, 22759, 2154, 33516, 28657, 21489, 1652, 1268, 27894, 28918, 44278, 11263}\n",
      "Austen, Jane{141, 158, 161, 22953, 22954, 42671, 946, 22962, 22963, 22964, 37431, 1212, 26301, 1342, 20682, 20686, 21839, 20687, 42078, 105, 121, 31100, 19839}\n",
      "Poe, Edgar Allan{14082, 6557, 932, 32037, 50852, 9511, 9512, 9513, 12714, 9514, 45484, 9515, 9516, 10031, 1063, 1064, 1065, 25525, 8893, 55749, 23901, 1062, 17192, 2147, 2148, 2149, 2150, 2151, 28908, 51060}\n",
      "Henry, O.{8962, 2851, 1444, 13094, 3815, 22440, 1725, 22442, 3707, 1805, 1646, 2776, 1583, 2295, 7256, 2777, 1595, 2141}\n",
      "Melville, Herman{2694, 2701, 15, 13720, 13721, 34970, 23969, 12841, 9268, 9269, 8118, 21816, 2489, 9146, 9147, 15422, 4045, 10712, 11231, 12384, 53861, 1900, 28656, 15859, 28794}\n",
      "Hawthorne, Nathaniel{512, 513, 9218, 9217, 9216, 9219, 9220, 9221, 9222, 9223, 9224, 9225, 9226, 9227, 9228, 7183, 9229, 9230, 9234, 9235, 9236, 9237, 9238, 9232, 9240, 9241, 9242, 9243, 9244, 9245, 9246, 9247, 9248, 33, 9249, 9250, 9251, 9252, 9253, 9254, 9255, 9256, 9257, 9258, 35377, 77, 2181, 2182, 30376, 7876, 7877, 7878, 7879, 7880, 7881, 7372, 2081, 8429, 37625, 9209, 25344, 51995, 39716, 15697, 41309, 9231, 9233, 1916, 1926, 31112, 13707, 9239, 41368, 8088, 8089, 8091, 8090, 7085, 7119, 976, 508, 9201, 32242, 9202, 9203, 9204, 9205, 9207, 9208, 9206, 9210, 9211, 9212, 9213, 9214, 9215}\n",
      "Wharton, Edith{39042, 57347, 267, 54932, 41855, 283, 24348, 24349, 284, 541, 24350, 24351, 4514, 4517, 4518, 295, 4519, 166, 11052, 40367, 306, 4533, 311, 9277, 9278, 9279, 9280, 9281, 9282, 9283, 9284, 4549, 4550, 9285, 9286, 9287, 9288, 9289, 9290, 9291, 9292, 9293, 24133, 26458, 53495, 7516, 7517, 11104, 24131, 9190, 4327, 1263, 19191, 24132, 8958, 55807}\n"
     ]
    }
   ],
   "source": [
    "# Show all english works of different authors.\n",
    "def Intersection(lst1, lst2):\n",
    "    return set(lst1).intersection(set(lst2))\n",
    "\n",
    "ShakespeareWilliam = Intersection(get_etexts('author', 'Shakespeare, William'), get_etexts('language', 'en'))\n",
    "DickensCharles = Intersection(get_etexts('author', 'Dickens, Charles'), get_etexts('language', 'en'))\n",
    "TwainMark = Intersection(get_etexts('author', 'Twain, Mark'), get_etexts('language', 'en'))\n",
    "VerneJules = Intersection(get_etexts('author', 'Verne, Jules'), get_etexts('language', 'en'))\n",
    "AustenJane = Intersection(get_etexts('author', 'Austen, Jane'), get_etexts('language', 'en'))\n",
    "PoeEdgarAllan = Intersection(get_etexts('author', 'Poe, Edgar Allan'), get_etexts('language', 'en'))\n",
    "HenryO = Intersection(get_etexts('author', 'Henry, O.'), get_etexts('language', 'en'))\n",
    "MelvilleHerman = Intersection(get_etexts('author', 'Melville, Herman'), get_etexts('language', 'en'))\n",
    "HawthorneNathaniel = Intersection(get_etexts('author', 'Hawthorne, Nathaniel'), get_etexts('language', 'en'))\n",
    "WhartonEdith = Intersection(get_etexts('author', 'Wharton, Edith'), get_etexts('language', 'en'))\n",
    "\n",
    "\n",
    "print('Shakespeare, William'+str(ShakespeareWilliam))\n",
    "print('Dickens, Charles'+str(DickensCharles))\n",
    "print('Twain, Mark'+str(TwainMark))\n",
    "print('Verne, Jules'+str(VerneJules))\n",
    "print('Austen, Jane'+str(AustenJane))\n",
    "print('Poe, Edgar Allan'+str(PoeEdgarAllan))\n",
    "print('Henry, O.'+str(HenryO))\n",
    "print('Melville, Herman'+str(MelvilleHerman))\n",
    "print('Hawthorne, Nathaniel'+str(HawthorneNathaniel))\n",
    "print('Wharton, Edith'+str(WhartonEdith))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shakespeare, William chosen works:[49007  1799  1784  1795 50095 12724  2254  1783  1516  1045]\n",
      "Dickens, Charles chosen works:[ 9717  9711  9696  9706  1423  9695 49683   588   810  9738]\n",
      "Twain, Mark chosen works:[ 7156  9016 33077  8481  9007  9033  9031  7155  7159  3251]\n",
      "Verne, Jules chosen works:[ 8980 22759  8984 12051 33516 46597 11556  8993  8979  8983]\n",
      "Austen, Jane chosen works:[22963 31100 26301 42671   141 22962  1212 31100 19839 20686]\n",
      "Poe, Edgar Allan chosen works:[12714  1062  9516  9511  2147 14082 28908  2151  9513  9515]\n",
      "Henry, O. chosen works:[ 3707  1583 22440  8962 22442  2776  2777  2141  2295  8962]\n",
      "Melville, Herman chosen works:[ 9268  1900  2489 34970 15859  2694 12841 21816  1900 28656]\n",
      "Hawthorne, Nathaniel chosen works:[41368  9226  9219   512  9244  9243  9239  8091  9204  2182]\n",
      "Wharton, Edith chosen works:[  284 24132  4514   283 55807 39042  9282  9281 24349 24351]\n"
     ]
    }
   ],
   "source": [
    "# Randomly choose 10 works from each author to form the corpus(training data).\n",
    "print('Shakespeare, William chosen works:'+str(np.random.RandomState(15).choice(list(ShakespeareWilliam), size=10)))\n",
    "print('Dickens, Charles chosen works:'+str(np.random.RandomState(15).choice(list(DickensCharles), size=10)))\n",
    "print('Twain, Mark chosen works:'+str(np.random.RandomState(15).choice(list(TwainMark), size=10)))\n",
    "print('Verne, Jules chosen works:'+str(np.random.RandomState(15).choice(list(VerneJules), size=10)))\n",
    "print('Austen, Jane chosen works:'+str(np.random.RandomState(15).choice(list(AustenJane), size=10)))\n",
    "print('Poe, Edgar Allan chosen works:'+str(np.random.RandomState(15).choice(list(PoeEdgarAllan), size=10)))\n",
    "print('Henry, O. chosen works:'+str(np.random.RandomState(15).choice(list(HenryO), size=10)))\n",
    "print('Melville, Herman chosen works:'+str(np.random.RandomState(15).choice(list(MelvilleHerman), size=10)))\n",
    "print('Hawthorne, Nathaniel chosen works:'+str(np.random.RandomState(15).choice(list(HawthorneNathaniel), size=10)))\n",
    "print('Wharton, Edith chosen works:'+str(np.random.RandomState(15).choice(list(WhartonEdith), size=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate training data(10 works from each author and some works are replaced by the other works from the same author.)\n",
    "y_train = [i for i in authors_id for r in range(10)]\n",
    "x_train = []\n",
    "x_train.extend([strip_headers(load_etext(49007)).strip(),strip_headers(load_etext(1799)).strip(),strip_headers(load_etext(1784)).strip(),strip_headers(load_etext(1795)).strip(),strip_headers(load_etext(50095)).strip(), strip_headers(load_etext(1536)).strip(),strip_headers(load_etext(2254)).strip(),strip_headers(load_etext(1783)).strip(),strip_headers(load_etext(1516)).strip(),strip_headers(load_etext(1045)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(1023)).strip(),strip_headers(load_etext(46)).strip(),strip_headers(load_etext(19505)).strip(),strip_headers(load_etext(564)).strip(),strip_headers(load_etext(1423)).strip(),strip_headers(load_etext(580)).strip(),strip_headers(load_etext(49683)).strip(),strip_headers(load_etext(588)).strip(),strip_headers(load_etext(810)).strip(),strip_headers(load_etext(46675)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(7156)).strip(),strip_headers(load_etext(2572)).strip(),strip_headers(load_etext(33077)).strip(),strip_headers(load_etext(8481)).strip(),strip_headers(load_etext(19987)).strip(),strip_headers(load_etext(1044)).strip(),strip_headers(load_etext(7193)).strip(),strip_headers(load_etext(7155)).strip(),strip_headers(load_etext(7159)).strip(),strip_headers(load_etext(3251)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(19362)).strip(),strip_headers(load_etext(22759)).strip(),strip_headers(load_etext(9618)).strip(),strip_headers(load_etext(12051)).strip(),strip_headers(load_etext(33516)).strip(),strip_headers(load_etext(46597)).strip(),strip_headers(load_etext(11556)).strip(),strip_headers(load_etext(3091)).strip(),strip_headers(load_etext(26658)).strip(),strip_headers(load_etext(2083)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(158)).strip(),strip_headers(load_etext(31100)).strip(),strip_headers(load_etext(161)).strip(),strip_headers(load_etext(42671)).strip(),strip_headers(load_etext(141)).strip(),strip_headers(load_etext(946)).strip(),strip_headers(load_etext(1212)).strip(),strip_headers(load_etext(31100)).strip(),strip_headers(load_etext(19839)).strip(),strip_headers(load_etext(37431)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(932)).strip(),strip_headers(load_etext(1062)).strip(),strip_headers(load_etext(32037)).strip(),strip_headers(load_etext(50852)).strip(),strip_headers(load_etext(2147)).strip(),strip_headers(load_etext(14082)).strip(),strip_headers(load_etext(45484)).strip(),strip_headers(load_etext(2151)).strip(),strip_headers(load_etext(10031)).strip(),strip_headers(load_etext(1063)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(3707)).strip(),strip_headers(load_etext(1583)).strip(),strip_headers(load_etext(2851)).strip(),strip_headers(load_etext(1444)).strip(),strip_headers(load_etext(13094)).strip(),strip_headers(load_etext(2776)).strip(),strip_headers(load_etext(2777)).strip(),strip_headers(load_etext(2141)).strip(),strip_headers(load_etext(2295)).strip(),strip_headers(load_etext(3815)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(2701)).strip(),strip_headers(load_etext(1900)).strip(),strip_headers(load_etext(2489)).strip(),strip_headers(load_etext(34970)).strip(),strip_headers(load_etext(15859)).strip(),strip_headers(load_etext(2694)).strip(),strip_headers(load_etext(12841)).strip(),strip_headers(load_etext(21816)).strip(),strip_headers(load_etext(1900)).strip(),strip_headers(load_etext(28656)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(41368)).strip(),strip_headers(load_etext(9226)).strip(),strip_headers(load_etext(9219)).strip(),strip_headers(load_etext(512)).strip(),strip_headers(load_etext(9244)).strip(),strip_headers(load_etext(9243)).strip(),strip_headers(load_etext(9239)).strip(),strip_headers(load_etext(8091)).strip(),strip_headers(load_etext(9204)).strip(),strip_headers(load_etext(2182)).strip()])\n",
    "x_train.extend([strip_headers(load_etext(284)).strip(),strip_headers(load_etext(24132)).strip(),strip_headers(load_etext(4514)).strip(),strip_headers(load_etext(283)).strip(),strip_headers(load_etext(55807)).strip(),strip_headers(load_etext(39042)).strip(),strip_headers(load_etext(57347)).strip(),strip_headers(load_etext(267)).strip(),strip_headers(load_etext(29349)).strip(),strip_headers(load_etext(24351)).strip()])\n",
    "# Randomly shuffle the train data\n",
    "train = pd.DataFrame(\n",
    "    {'text': x_train,\n",
    "     'author': y_train\n",
    "     })\n",
    "train = train.sample(frac=1)\n",
    "x_train = train['text'].values\n",
    "y_train = train['author'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate test data(3 works from each author and some works are replaced by the other works from the same author.)\n",
    "y_test = [i for i in authors_id for r in range(3)]\n",
    "x_test = []\n",
    "x_test.extend([strip_headers(load_etext(1537)).strip(),strip_headers(load_etext(23042)).strip(),strip_headers(load_etext(23043)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(98)).strip(),strip_headers(load_etext(43111)).strip(),strip_headers(load_etext(644)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(7194)).strip(),strip_headers(load_etext(7195)).strip(),strip_headers(load_etext(19484)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(1698)).strip(),strip_headers(load_etext(3748)).strip(),strip_headers(load_etext(164)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(1342)).strip(),strip_headers(load_etext(42078)).strip(),strip_headers(load_etext(21839)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(1064)).strip(),strip_headers(load_etext(1065)).strip(),strip_headers(load_etext(25525)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(1725)).strip(),strip_headers(load_etext(1805)).strip(),strip_headers(load_etext(1646)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(15)).strip(),strip_headers(load_etext(13720)).strip(),strip_headers(load_etext(13721)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(512)).strip(),strip_headers(load_etext(513)).strip(),strip_headers(load_etext(9218)).strip()])\n",
    "x_test.extend([strip_headers(load_etext(54932)).strip(),strip_headers(load_etext(41855)).strip(),strip_headers(load_etext(24348)).strip()])\n",
    "# Randomly shuffle the test data\n",
    "test = pd.DataFrame(\n",
    "    {'text': x_test,\n",
    "     'author': y_test\n",
    "     })\n",
    "test = test.sample(frac=1)\n",
    "x_test = test['text'].values\n",
    "y_test = test['author'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Remove stopwords, stem the texts and create tfidf matrix. \n",
    "# Each unique word in our dictionary will correspond to a feature (descriptive feature).\n",
    "stemmer = SnowballStemmer(language=\"english\", ignore_stopwords=True)\n",
    "analyzer = CountVectorizer().build_analyzer()\n",
    "\n",
    "def stemmed_words(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "stem_vectorizer = CountVectorizer(analyzer=stemmed_words)\n",
    "x_train_stemmed = TfidfTransformer().fit_transform(stem_vectorizer.fit_transform(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.8 , 0.85, 0.8 , 0.65, 0.9 ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement SVM and use 5-fold cross-validation to evaluate the performance.\n",
    "clf_svm = LinearSVC(loss='hinge', penalty='l2', random_state=69)\n",
    "scores = cross_val_score(clf_svm, x_train_stemmed, y_train, scoring='accuracy', cv=5) \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Score for SVM:0.8\n"
     ]
    }
   ],
   "source": [
    "print ('Validation Score for SVM:'+str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95, 0.9 , 0.9 , 0.9 , 0.95])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implement Random Forest and use 5-fold cross-validation to evaluate the performance.\n",
    "clf_rf = RandomForestClassifier(n_estimators=1000, \n",
    "                                max_features='sqrt', \n",
    "                                max_depth=15,\n",
    "                                verbose=0,\n",
    "                                random_state=69)\n",
    "scores = cross_val_score(clf_rf, x_train_stemmed, y_train, scoring='accuracy', cv=5)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Score for Random Forest:0.9199999999999999\n"
     ]
    }
   ],
   "source": [
    "print ('Validation Score for Random Forest:'+str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score for SVM:0.9\n"
     ]
    }
   ],
   "source": [
    "# Build a pipeline and evaluate the performance of SVM on the test data.\n",
    "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
    "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
    "\n",
    "clf_svm = Pipeline([('vect', stemmed_count_vect),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf-svm', LinearSVC(loss='hinge', penalty='l2', random_state=69))])\n",
    "\n",
    "_ = clf_svm.fit(x_train, y_train)\n",
    "predicted_svm = clf_svm.predict(x_test)\n",
    "accuracy = np.mean(predicted_svm == y_test)\n",
    "\n",
    "print ('Test Score for SVM:'+str(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score for Random Forest:0.9\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of Random Forest on the test data.\n",
    "clf_rf = Pipeline([('vect', stemmed_count_vect),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf-rf', RandomForestClassifier(n_estimators=1000, \n",
    "                                max_features='sqrt', \n",
    "                                max_depth=15,\n",
    "                                verbose=0,\n",
    "                                random_state=69))])\n",
    "\n",
    "_ = clf_rf.fit(x_train, y_train)\n",
    "predicted_rf = clf_rf.predict(x_test)\n",
    "accuracy = np.mean(predicted_rf == y_test)\n",
    "\n",
    "print ('Test Score for Random Forest:'+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have trained two models (SVM & Random Forest) and evaluate their performance by 1) cross-validation 2) test data. Next let's try $\\textbf{Grid Search}$ and train one more model($\\textbf{Naive Bayes}$). I use $\\textbf{majority vote}$ as our emsemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 10, 'max_features': 'sqrt', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Random Forest to select best parameters.\n",
    "rf = RandomForestClassifier(random_state = 69)\n",
    "rf_params = {\n",
    "             'n_estimators':[1000, 1500, 3000], \n",
    "             'max_depth': [10, 30, 50],\n",
    "             'max_features': ['log2', 'sqrt']\n",
    "}\n",
    "clf = GridSearchCV(rf, rf_params, n_jobs=-1)\n",
    "clf.fit(x_train_stemmed, y_train)\n",
    "print (clf.best_params_)\n",
    "\n",
    "rf_max_depth = clf.best_params_['max_depth']\n",
    "rf_max_features = clf.best_params_['max_features']\n",
    "rf_n_estimators = clf.best_params_['n_estimators']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Naive Bayes to select best parameters.\n",
    "nb = MultinomialNB()\n",
    "nb_params = {\n",
    "             'alpha':[0.1, 0.5, 1, 2], \n",
    "}\n",
    "clf = GridSearchCV(nb, nb_params, n_jobs=-1)\n",
    "clf.fit(x_train_stemmed, y_train)\n",
    "print (clf.best_params_)\n",
    "\n",
    "nb_alpha = clf.best_params_['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1}\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Support Vector Classifier to select best parameters.\n",
    "svc = LinearSVC()\n",
    "svc_params = {\n",
    "    'C':[0.025, 0.05, 0.1, 0.5, 1]}\n",
    "clf = GridSearchCV(svc, svc_params)\n",
    "clf.fit(x_train_stemmed, y_train)\n",
    "print (clf.best_params_)\n",
    "\n",
    "svc_C = clf.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score for Majority Vote:0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# Use the parameters selected from grid search and use majority as ensemble method\n",
    "clfs = []\n",
    "model1 = RandomForestClassifier(n_jobs=-1,\n",
    "                                n_estimators=rf_n_estimators,\n",
    "                                max_depth=rf_max_depth,\n",
    "                                min_samples_leaf=2,\n",
    "                                max_features=rf_max_features,\n",
    "                                verbose=0,\n",
    "                                random_state=69)\n",
    "clfs.append(('RandomForest', model1))\n",
    "model2 = MultinomialNB(alpha=0.1)\n",
    "clfs.append(('NaiveBayes', model2))\n",
    "model3 = LinearSVC(C=1)\n",
    "clfs.append(('SVM', model3))\n",
    "# create the ensemble model\n",
    "mv = Pipeline([('vect', stemmed_count_vect),\n",
    "                    ('tfidf', TfidfTransformer()),\n",
    "                    ('clf-svm', VotingClassifier(clfs, voting='hard'))])\n",
    "\n",
    "_ = mv.fit(x_train, y_train)\n",
    "predicted = mv.predict(x_test)\n",
    "accuracy = np.mean(predicted == y_test)\n",
    "\n",
    "print ('Test Score for Majority Vote:'+str(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using Majority vote and grid search, my model outperforms the single SVM or Random Forest algorithm. Now let's go deeper by using $\\textbf{Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM).}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokennize the texts and pad the sequences\n",
    "tokenizer = Tokenizer(num_words=10000) # use the most common 20000 words \n",
    "tokenizer.fit_on_texts(x_train) # calculate the word frequencies\n",
    "sequences = tokenizer.texts_to_sequences(x_train) # transform the text into numerical tokens\n",
    "data = pad_sequences(sequences, maxlen=300) # \"pad” the sequences so that the training examples are the same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60 samples, validate on 40 samples\n",
      "Epoch 1/5\n",
      "60/60 [==============================] - 6s 98ms/step - loss: 4.6034 - acc: 0.0500 - val_loss: 4.5959 - val_acc: 0.1250\n",
      "Epoch 2/5\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 4.5848 - acc: 0.5667 - val_loss: 4.5827 - val_acc: 0.2250\n",
      "Epoch 3/5\n",
      "60/60 [==============================] - 4s 64ms/step - loss: 4.5589 - acc: 0.7500 - val_loss: 4.5538 - val_acc: 0.2250\n",
      "Epoch 4/5\n",
      "60/60 [==============================] - 3s 58ms/step - loss: 4.5022 - acc: 0.6667 - val_loss: 4.3886 - val_acc: 0.0750\n",
      "Epoch 5/5\n",
      "60/60 [==============================] - 3s 57ms/step - loss: 4.1668 - acc: 0.2833 - val_loss: 3.7636 - val_acc: 0.0750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a27e03e80>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM)\n",
    "random.seed(69)\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 256, input_length=300))\n",
    "model.add(LSTM(256, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(100, activation='sigmoid'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(data, np.array(y_train), validation_split=0.4, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With the accuracy of 0.75, I think we need more data to train this complicated neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
